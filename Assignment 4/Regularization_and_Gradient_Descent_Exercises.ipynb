{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Gradient Descent Exercises\n",
    "![UnderOverFit.png](Assets/UnderOverFit.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "- Explain cost functions, regularization, feature selection, and hyper-parameters\n",
    "- Summarize complex statistical optimization algorithms like gradient descent and its application to linear regression\n",
    "- Apply Intel® Extension for Scikit-learn* to leverage underlying compute capabilities of hardware\n",
    "\n",
    "# scikit-learn* \n",
    "\n",
    "Frameworks provide structure that Data Scientists use to build code. Frameworks are more than just libraries, because in addition to callable code, frameworks influence how code is written. \n",
    "\n",
    "A main virtue of using an optimized framework is that code runs faster. Code that runs faster is just generally more convenient but when we begin looking at applied data science and AI models, we can see more material benefits. Here you will see how optimization, particularly hyperparameter optimization can benefit more than just speed. \n",
    "\n",
    "These exercises will demonstrate how to apply **the Intel® Extension for Scikit-learn*,** a seamless way to speed up your Scikit-learn application. The acceleration is achieved through the use of the Intel® oneAPI Data Analytics Library (oneDAL). Patching is the term used to extend scikit-learn with Intel optimizations and makes it a well-suited machine learning framework for dealing with real-life problems. \n",
    "\n",
    "To get optimized versions of many Scikit-learn algorithms using a patch() approach consisting of adding these lines of code prior to importing sklearn: \n",
    "\n",
    "- **from sklearnex import patch_sklearn**\n",
    "- **patch_sklearn()**\n",
    "\n",
    "## This exercise relies on installation of  Intel® Extension for Scikit-learn*\n",
    "\n",
    "If you have not already done so, follow the instructions from Week 1 for instructions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will begin with a short tutorial on regression, polynomial features, and regularization based on a very simple, sparse data set that contains a column of `x` data and associated `y` noisy data. The data file is called `X_Y_Sinusoid_Data.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:14:31.702476Z",
     "start_time": "2021-09-24T17:14:30.515445Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearnex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m data_path \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearnex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m patch_sklearn\n\u001b[0;32m      6\u001b[0m patch_sklearn()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PolynomialFeatures\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearnex'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "data_path = ['../data']\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 1\n",
    "\n",
    "* Import the data. \n",
    "\n",
    "* Also generate approximately 100 equally spaced x data points over the range of 0 to 1. Using these points, calculate the y-data which represents the \"ground truth\" (the real function) from the equation: $y = sin(2\\pi x)$\n",
    "\n",
    "* Plot the sparse data (`x` vs `y`) and the calculated (\"real\") data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:14:31.718026Z",
     "start_time": "2021-09-24T17:14:31.704449Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#filepath = os.sep.join(data_path + ['X_Y_Sinusoid_Data.csv'])\n",
    "#data = pd.read_csv(filepath)\n",
    "\n",
    "#X_real = np.linspace(0, 1.0, 100)\n",
    "#Y_real = np.sin(2 * np.pi * X_real)\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import pandas as pd # Import the pandas library\n",
    "\n",
    "# Data path\n",
    "data_path = '/content/X_Y_Sinusoid_Data.csv'\n",
    "\n",
    "# Required imports\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data into a pandas DataFrame called 'data'\n",
    "data = pd.read_csv(data_path) # Load data from CSV file\n",
    "\n",
    "# Display the count of each data type\n",
    "print(data.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:14:31.997999Z",
     "start_time": "2021-09-24T17:14:31.983994Z"
    }
   },
   "outputs": [],
   "source": [
    "#sns.set_style('white')\n",
    "#sns.set_context('talk')\n",
    "#sns.set_palette('dark')\n",
    "\n",
    "# Plot of the noisy (sparse)\n",
    "#ax = data.set_index('x')['y'].plot(ls='', marker='o', label='data')\n",
    "#ax.plot(X_real, Y_real, ls='--', marker='', label='real function')\n",
    "\n",
    "#ax.legend()\n",
    "#ax.set(xlabel='x data', ylabel='y data');\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To ensure inline plots in Jupyter Notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Set up visualization styles\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "sns.set_palette('dark')\n",
    "\n",
    "# Plot the noisy data (sparse data from the CSV file)\n",
    "# Generate equally spaced x data points over the range [0, 1]\n",
    "X_real = np.linspace(0, 1.0, 100)\n",
    "\n",
    "# Calculate y values for the \"real\" function using y = sin(2πx)\n",
    "Y_real = np.sin(2 * np.pi * X_real)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = data.set_index('x')['y'].plot(ls='', marker='o', label='Noisy Data')\n",
    "\n",
    "# Plot the real data (ground truth function)\n",
    "ax.plot(X_real, Y_real, ls='--', marker='', label='Real Function')\n",
    "\n",
    "# Add labels and title\n",
    "ax.legend()\n",
    "ax.set(xlabel='x data', ylabel='y data', title='Noisy Data vs Real Function')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "\n",
    "* Using the `PolynomialFeatures` class from Scikit-learn's preprocessing library, create 20th order polynomial features.\n",
    "* Fit this data using linear regression. \n",
    "* Plot the resulting predicted value compared to the calculated data.\n",
    "\n",
    "Note that `PolynomialFeatures` requires either a dataframe (with one column, not a Series) or a 2D array of dimension (`X`, 1), where `X` is the length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:14:32.013995Z",
     "start_time": "2021-09-24T17:14:31.998996Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Ensure that the x data is in the correct shape (2D array)\n",
    "X_data = data['x'].values.reshape(-1, 1)\n",
    "\n",
    "# Initialize the PolynomialFeatures class with degree 20\n",
    "poly = PolynomialFeatures(degree=20)\n",
    "\n",
    "# Transform the original data to include polynomial features\n",
    "X_poly = poly.fit_transform(X_data)\n",
    "\n",
    "# Initialize and fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, data['y'])\n",
    "\n",
    "# Generate predicted values using the fitted model\n",
    "Y_pred = model.predict(X_poly)\n",
    "\n",
    "# Plot the noisy data and real function\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the noisy (sparse) data\n",
    "ax = data.set_index('x')['y'].plot(ls='', marker='o', label='Noisy Data')\n",
    "\n",
    "# Plot the real function (ground truth)\n",
    "ax.plot(X_real, Y_real, ls='--', marker='', label='Real Function')\n",
    "\n",
    "# Plot the predicted polynomial regression values\n",
    "ax.plot(X_data, Y_pred, ls='-', marker='', label='Polynomial Regression (degree 20)')\n",
    "\n",
    "# Add labels and title\n",
    "ax.legend()\n",
    "ax.set(xlabel='x data', ylabel='y data', title='Polynomial Regression vs Real Function')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "* Perform the regression on using the data with polynomial features using ridge regression ($\\alpha$=0.001) and lasso regression ($\\alpha$=0.0001). \n",
    "* Plot the results, as was done in Question 1. \n",
    "* Also plot the magnitude of the coefficients obtained from these regressions, and compare them to those obtained from linear regression in the previous question. The linear regression coefficients will likely need a separate plot (or their own y-axis) due to their large magnitude. \n",
    "\n",
    "What does the comparatively large magnitude of the data tell you about the role of regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:14:32.028994Z",
     "start_time": "2021-09-24T17:14:32.015995Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mute the sklearn warning about regularization\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='sklearn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:14:32.044995Z",
     "start_time": "2021-09-24T17:14:32.029995Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's look at the absolute value of coefficients for each model\n",
    "\n",
    "#coefficients = pd.DataFrame()\n",
    "#update coefficients with all models\n",
    "\n",
    "#describe coefficients\n",
    "# Huge difference in scale between non-regularized vs regularized regression\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# Initialize Ridge and Lasso regression models with specified alphas\n",
    "ridge = Ridge(alpha=0.001)\n",
    "lasso = Lasso(alpha=0.0001)\n",
    "\n",
    "# Fit the models on the polynomial features\n",
    "ridge.fit(X_poly, data['y'])\n",
    "lasso.fit(X_poly, data['y'])\n",
    "\n",
    "# Generate predictions for Ridge and Lasso\n",
    "Y_ridge_pred = ridge.predict(X_poly)\n",
    "Y_lasso_pred = lasso.predict(X_poly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:14:32.060994Z",
     "start_time": "2021-09-24T17:14:32.046996Z"
    }
   },
   "outputs": [],
   "source": [
    "#colors = sns.color_palette()\n",
    "\n",
    "# Setup the dual y-axes\n",
    "#ax1 = plt.axes()\n",
    "#ax2 = ax1.twinx()\n",
    "# Plot the noisy data, real function, and regression results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the noisy (sparse) data\n",
    "ax = data.set_index('x')['y'].plot(ls='', marker='o', label='Noisy Data')\n",
    "\n",
    "# Plot the real function (ground truth)\n",
    "ax.plot(X_real, Y_real, ls='--', marker='', label='Real Function')\n",
    "\n",
    "# Plot the predictions for Linear Regression, Ridge, and Lasso\n",
    "ax.plot(X_data, Y_pred, ls='-', marker='', label='Polynomial Regression (degree 20)')\n",
    "ax.plot(X_data, Y_ridge_pred, ls='-', marker='', label='Ridge Regression (α=0.001)')\n",
    "ax.plot(X_data, Y_lasso_pred, ls='-', marker='', label='Lasso Regression (α=0.0001)')\n",
    "\n",
    "# Add labels and title\n",
    "ax.legend()\n",
    "ax.set(xlabel='x data', ylabel='y data', title='Comparing Polynomial, Ridge, and Lasso Regression')\n",
    "plt.show()\n",
    "# Plot the magnitudes of the coefficients from Linear, Ridge, and Lasso regressions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Linear Regression Coefficients\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.abs(model.coef_), label='Linear Regression Coefficients', color='blue')\n",
    "\n",
    "# Plot Ridge Regression Coefficients\n",
    "plt.plot(np.abs(ridge.coef_), label='Ridge Regression Coefficients', color='red')\n",
    "\n",
    "# Plot Lasso Regression Coefficients\n",
    "plt.plot(np.abs(lasso.coef_), label='Lasso Regression Coefficients', color='green')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Coefficient Magnitude')\n",
    "plt.title('Magnitude of Coefficients from Linear, Ridge, and Lasso')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "For the remaining questions, we will be working with the [data set](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) from last lesson, which is based on housing prices in Ames, Iowa. There are an extensive number of features--see the exercises from week three for a discussion of these features.\n",
    "\n",
    "To begin:\n",
    "\n",
    "* Import the data with Pandas, remove any null values, and one hot encode categoricals. Either Scikit-learn's feature encoders or Pandas `get_dummies` method can be used.\n",
    "* Split the data into train and test sets. \n",
    "* Log transform skewed features. \n",
    "* Scaling can be attempted, although it can be interesting to see how well regularization works without scaling features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:14:32.075996Z",
     "start_time": "2021-09-24T17:14:32.061995Z"
    }
   },
   "outputs": [],
   "source": [
    "#filepath = os.sep.join(data_path + ['Ames_Housing_Sales.csv'])\n",
    "#data1 = pd.read_csv(filepath, sep=',')\n",
    "#data = data1.dropna()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Ames housing dataset\n",
    "# Replace the path with the actual dataset path\n",
    "data_path = '/content/X_Y_Sinusoid_Data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T02:40:49.956043Z",
     "start_time": "2017-03-09T21:40:49.950878-05:00"
    }
   },
   "source": [
    "Create a list of categorial data and one-hot encode. Pandas one-hot encoder (`get_dummies`) works well with data that is defined as a categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T17:01:05.304547Z",
     "start_time": "2017-03-10T12:01:05.231567-05:00"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check for null values in the dataset\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values (alternative: fillna())\n",
    "df = df.dropna()\n",
    "\n",
    "# Alternatively, you can fill missing values with the median or mode if necessary\n",
    "# df = df.fillna(df.median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, split the data in train and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T07:56:47.954727Z",
     "start_time": "2021-09-17T07:56:47.939629Z"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encode categorical features using pandas get_dummies method\n",
    "df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Alternatively, using OneHotEncoder from scikit-learn if desired\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "# categorical_features = df.select_dtypes(include=['object']).columns\n",
    "# encoded_categoricals = encoder.fit_transform(df[categorical_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of columns that have skewed features--a log transformation can be applied to them. Note that this includes the `SalePrice`, our predictor. However, let's keep that one as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:14:32.091995Z",
     "start_time": "2021-09-24T17:14:32.076995Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of float colums to check for skewing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T17:01:10.689590Z",
     "start_time": "2017-03-10T12:01:10.609841-05:00"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check skewness of the numerical features\n",
    "numeric_features = df_encoded.select_dtypes(include=['float64', 'int64']).columns\n",
    "skewness = df_encoded[numeric_features].skew()\n",
    "\n",
    "# Identify skewed features\n",
    "skewed_features = skewness[skewness > 0.75].index\n",
    "\n",
    "# Apply log transformation to skewed features\n",
    "for feature in skewed_features:\n",
    "    df_encoded[feature] = np.log1p(df_encoded[feature])  # log1p handles log(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform all the columns where the skew is greater than 0.75, excluding \"SalePrice\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:14:32.107996Z",
     "start_time": "2021-09-24T17:14:32.093996Z"
    }
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: Let's look at what happens to one of these features, when we apply np.log1p visually.\n",
    "# Assuming 'y' is the target variable (house price)\n",
    "X = df_encoded.drop('y', axis=1)\n",
    "y = df_encoded['y']\n",
    "\n",
    "# Split the data into train and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:14:32.123995Z",
     "start_time": "2021-09-24T17:14:32.109998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mute the setting wtih a copy warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate features from predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T17:01:21.972625Z",
     "start_time": "2017-03-10T12:01:21.957050-05:00"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Optionally, scale the features if desired\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and test sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:11:03.256453",
     "start_time": "2017-02-21T09:11:03.241117"
    }
   },
   "source": [
    "## Question 5\n",
    "\n",
    "* Write a function **`rmse`** that takes in truth and prediction values and returns the root-mean-squared error. Use sklearn's `mean_squared_error`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T07:56:48.018153Z",
     "start_time": "2021-09-17T07:56:48.003051Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Squared Error (RMSE) between true and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: array-like, True values\n",
    "    - y_pred: array-like, Predicted values\n",
    "\n",
    "    Returns:\n",
    "    - rmse: float, Root Mean Squared Error\n",
    "    \"\"\"\n",
    "    # Calculate Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    # Return the square root of MSE to get RMSE\n",
    "    return np.sqrt(mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit a basic linear regression model\n",
    "* print the root-mean-squared error for this model\n",
    "* plot the predicted vs actual sale price based on the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T07:56:48.033436Z",
     "start_time": "2021-09-17T07:56:48.019033Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'y' is the target variable and you have a trained model\n",
    "# Replace 'model' with your actual model variable\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE using the actual and predicted values from the test set\n",
    "result = rmse(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE: {result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'model' is the trained LinearRegression model from Question 2\n",
    "# and 'poly' is the PolynomialFeatures object used for transformation\n",
    "\n",
    "# Transform the test data using the same PolynomialFeatures object\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Now make predictions using the transformed test data\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Calculate RMSE using the actual and predicted values from the test set\n",
    "result = rmse(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Ridge regression uses L2 normalization to reduce the magnitude of the coefficients. This can be helpful in situations where there is high variance. The regularization functions in Scikit-learn each contain versions that have cross-validation built in.\n",
    "\n",
    "* Fit a regular (non-cross validated) Ridge model to a range of $\\alpha$ values and plot the RMSE using the cross validated error function you created above.\n",
    "* Use $$[0.005, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 80]$$ as the range of alphas.\n",
    "* Then repeat the fitting of the Ridge models using the range of $\\alpha$ values from the prior section. Compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the `RidgeCV` method. It's not possible to get the alpha values for the models that weren't selected, unfortunately. The resulting error values and $\\alpha$ values are very similar to those obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T07:56:48.049335Z",
     "start_time": "2021-09-17T07:56:48.034493Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "X = df.drop(columns=['y'])  # Replace 'SalePrice' with your target column\n",
    "y = df['y']  # Replace with the target column\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the range of alphas\n",
    "alpha_values = [0.005, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 80]\n",
    "\n",
    "# Initialize a list to store RMSE values\n",
    "rmse_values = []\n",
    "\n",
    "# Fit Ridge regression models for each alpha and compute the RMSE\n",
    "for alpha in alpha_values:\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse_val = rmse(y_test, y_pred)\n",
    "    rmse_values.append(rmse_val)\n",
    "\n",
    "# Plot RMSE vs Alpha values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alpha_values, rmse_values, marker='o', linestyle='-', color='b')\n",
    "plt.xscale('log')  # Log scale for better visualization\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE for Ridge Regression with Different Alpha Values')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "\n",
    "Much like the `RidgeCV` function, there is also a `LassoCV` function that uses an L1 regularization function and cross-validation. L1 regularization will selectively shrink some coefficients, effectively performing feature elimination.\n",
    "\n",
    "The `LassoCV` function does not allow the scoring function to be set. However, the custom error function (`rmse`) created above can be used to evaluate the error on the final model.\n",
    "\n",
    "Similarly, there is also an elastic net function with cross validation, `ElasticNetCV`, which is a combination of L2 and L1 regularization.\n",
    "\n",
    "* Fit a Lasso model using cross validation and determine the optimum value for $\\alpha$ and the RMSE using the function created above. Note that the magnitude of $\\alpha$ may be different from the Ridge model.\n",
    "* Repeat this with the Elastic net model.\n",
    "* Compare the results via table and/or plot.\n",
    "\n",
    "Use the following alphas:  \n",
    "`[1e-5, 5e-5, 0.0001, 0.0005]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T07:56:48.064382Z",
     "start_time": "2021-09-17T07:56:48.051275Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Define the range of alphas\n",
    "alpha_values_lasso = [1e-5, 5e-5, 0.0001, 0.0005]\n",
    "\n",
    "# Fit LassoCV with cross-validation\n",
    "lasso_cv_model = LassoCV(alphas=alpha_values_lasso, cv=5)  # 5-fold cross-validation\n",
    "lasso_cv_model.fit(X_train, y_train)\n",
    "\n",
    "# Get the optimal alpha and the RMSE\n",
    "optimal_alpha_lasso = lasso_cv_model.alpha_\n",
    "y_pred_lasso = lasso_cv_model.predict(X_test)\n",
    "rmse_lasso = rmse(y_test, y_pred_lasso)\n",
    "\n",
    "print(f'Optimal Alpha from LassoCV: {optimal_alpha_lasso}')\n",
    "print(f'RMSE from LassoCV: {rmse_lasso}')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can determine how many of these features remain non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:50:13.053851",
     "start_time": "2017-02-21T09:50:13.047466"
    }
   },
   "outputs": [],
   "source": [
    "# LassoCV\n",
    "lasso_alphas = [1e-5, 5e-5, 0.0001, 0.0005]\n",
    "lasso_cv = LassoCV(alphas=lasso_alphas, cv=5)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "lasso_rmse = rmse(y_test, lasso_cv.predict(X_test))\n",
    "print(f\"Best alpha from LassoCV: {lasso_cv.alpha_:.5f}\")\n",
    "print(f\"Test RMSE: {lasso_rmse:.2f}\")\n",
    "print(f\"Number of non-zero coefficients: {np.sum(lasso_cv.coef_ != 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T12:03:06.013488",
     "start_time": "2017-02-16T12:03:06.007159"
    },
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "Now try the elastic net, with the same alphas as in Lasso, and l1_ratios between 0.1 and 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T07:56:48.080355Z",
     "start_time": "2021-09-17T07:56:48.066355Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ElasticNetCV\n",
    "enet_alphas = [1e-5, 5e-5, 0.0001, 0.0005]\n",
    "enet_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "enet_cv = ElasticNetCV(alphas=enet_alphas, l1_ratio=enet_ratios, cv=5)\n",
    "enet_cv.fit(X_train, y_train)\n",
    "enet_rmse = rmse(y_test, enet_cv.predict(X_test))\n",
    "print(f\"Best alpha from ElasticNetCV: {enet_cv.alpha_:.5f}\")\n",
    "print(f\"Best l1_ratio: {enet_cv.l1_ratio_:.1f}\")\n",
    "print(f\"Test RMSE: {enet_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the RMSE calculation from all models is easiest in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:14:32.139996Z",
     "start_time": "2021-09-24T17:14:32.124996Z"
    }
   },
   "outputs": [],
   "source": [
    "#labels = ['Linear', 'Ridge', 'Lasso', 'ElasticNet']\n",
    "# Compare models\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Linear', 'Ridge', 'Lasso', 'ElasticNet'],\n",
    "    'Test RMSE': [\n",
    "        rmse(y_test, lr.predict(X_test)),\n",
    "        rmse(y_test, ridge_cv.predict(X_test)),\n",
    "        lasso_rmse,\n",
    "        enet_rmse\n",
    "    ]\n",
    "})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a plot of actual vs predicted housing prices as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:53:54.142116",
     "start_time": "2017-02-21T09:53:53.857081"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating a comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['LassoCV', 'ElasticNetCV'],\n",
    "    'Optimal Alpha': [optimal_alpha_lasso, optimal_alpha_en],\n",
    "    'RMSE': [rmse_lasso, rmse_en]\n",
    "})\n",
    "\n",
    "print(comparison)\n",
    "\n",
    "# Plotting the comparison of RMSE for LassoCV and ElasticNetCV\n",
    "plt.figure(figsize=(8, 6))\n",
    "models = ['LassoCV', 'ElasticNetCV']\n",
    "rmse_values = [rmse_lasso, rmse_en]\n",
    "\n",
    "plt.bar(models, rmse_values, color=['blue', 'green'])\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Comparison of RMSE for LassoCV and ElasticNetCV')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Let's explore Stochastic gradient descent in this exercise.  \n",
    "Recall that Linear models in general are sensitive to scaling.\n",
    "However, SGD is *very* sensitive to scaling.  \n",
    "Moreover, a high value of learning rate can cause the algorithm to diverge, whereas a too low value may take too long to converge.\n",
    "\n",
    "* Fit a stochastic gradient descent model without a regularization penalty (the relevant parameter is `penalty`).\n",
    "* Now fit stochastic gradient descent models with each of the three penalties (L2, L1, Elastic Net) using the parameter values determined by cross validation above. \n",
    "* Do not scale the data before fitting the model.  \n",
    "* Compare the results to those obtained without using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T07:56:48.111249Z",
     "start_time": "2021-09-17T07:56:48.098089Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Step 1: Fit an SGD model without regularization (penalty=None)\n",
    "sgd_model_none = SGDRegressor(penalty=None, max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_model_none.fit(X_train, y_train) # Changed x_train to X_train\n",
    "\n",
    "# Calculate RMSE for SGD model without regularization\n",
    "y_pred_none = sgd_model_none.predict(X_test)  # Predict on the test set\n",
    "rmse_none = rmse(y_test, y_pred_none)  # Calculate RMSE using your custom function\n",
    "\n",
    "# Step 2: Fit SGD models with L2, L1, and ElasticNet penalties\n",
    "sgd_model_l2 = SGDRegressor(penalty='l2', alpha=optimal_alpha_en, max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_model_l2.fit(X_train, y_train) # Changed x_train to X_train\n",
    "\n",
    "sgd_model_l1 = SGDRegressor(penalty='l1', alpha=optimal_alpha_lasso, max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_model_l1.fit(X_train, y_train) # Changed x_train to X_train\n",
    "\n",
    "sgd_model_en = SGDRegressor(penalty='elasticnet', alpha=optimal_alpha_en, l1_ratio=0.5, max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_model_en.fit(X_train, y_train) # Changed x_train to X_train\n",
    "\n",
    "# Predictions and RMSE for each model\n",
    "y_pred_l2 = sgd_model_l2.predict(X_test)\n",
    "rmse_l2 = rmse(y_test, y_pred_l2)\n",
    "\n",
    "y_pred_l1 = sgd_model_l1.predict(X_test)\n",
    "rmse_l1 = rmse(y_test, y_pred_l1)\n",
    "\n",
    "y_pred_en = sgd_model_en.predict(X_test)\n",
    "rmse_en = rmse(y_test, y_pred_en)\n",
    "\n",
    "# Step 3: Compare the RMSE results\n",
    "comparison_sgd = pd.DataFrame({\n",
    "    'Model': ['SGD without regularization', 'SGD with L2 (Ridge)', 'SGD with L1 (Lasso)', 'SGD with ElasticNet'],\n",
    "    'RMSE': [rmse_none, rmse_l2, rmse_l1, rmse_en]\n",
    "})\n",
    "\n",
    "print(comparison_sgd)\n",
    "\n",
    "# Step 4: Plot the comparison of RMSE\n",
    "plt.figure(figsize=(8, 6))\n",
    "models = ['SGD without regularization', 'SGD with L2 (Ridge)', 'SGD with L1 (Lasso)', 'SGD with ElasticNet']\n",
    "rmse_values = [rmse_none, rmse_l2, rmse_l1, rmse_en]\n",
    "\n",
    "plt.bar(models, rmse_values, color=['blue', 'green', 'red', 'purple'])\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Comparison of RMSE for Different SGD Models')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
